{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d804ed0e-c6b0-41ae-a9a3-7040fcc6dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "!pip install PyPDF2 openai lancedb ipywidgets\n",
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import lancedb\n",
    "from ipywidgets import Text, Button, VBox, Output\n",
    "from openai import OpenAI\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = \"ENTER_YOUR_API_KEY\"\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a5f605-8497-48a9-ac50-25ae5e4f49ae",
   "metadata": {},
   "source": [
    "Extracts text from a PDF file by reading each page sequentially and combines it into a single string, then splits this text into overlapping chunks of specified size for easier processing or analysis.\n",
    "\n",
    "Insert your own OpenAI key above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b31e09b-3ac2-4a85-b3d7-856cfc3516a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "def extract_text_from_pdf(file_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=4000, overlap=256):\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks based on token count.\n",
    "    \n",
    "    This uses the tokenizer associated with text-embedding-ada-002 \n",
    "    (encoding \"cl100k_base\") and sets the chunk size to 8000 tokens \n",
    "    (nearly the full context window) with an overlap of 512 tokens.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to be split.\n",
    "        chunk_size (int): Maximum number of tokens per chunk. Default is 8000.\n",
    "        overlap (int): Number of tokens to overlap between chunks. Default is 512.\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    text_splitter = TokenTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        encoding_name=\"cl100k_base\"  # the encoding used by ada-002\n",
    "    )\n",
    "    return text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eadebe7-55ba-4efd-853e-f52a13623717",
   "metadata": {},
   "source": [
    "This code processes multiple PDF files by extracting text from each, chunking the content, and organizing it into a structured list of dictionaries where each dictionary contains a unique identifier, source file, text chunk, and company code derived from the filename.\n",
    "\n",
    "If you don't have the documents, you can go to sec.gov > view as html > print > save as pdf (https://www.sec.gov/ix?doc=/Archives/edgar/data/0000080661/000008066124000007/pgr-20231231.htm). Important to keep names as described in the files list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d3fedb07-a06d-4f8e-834c-b803dee822e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks from all PDFs: 161\n"
     ]
    }
   ],
   "source": [
    "# List of expected PDF filenames and their corresponding company codes will be derived from the filename.\n",
    "pdf_files = [\"all.pdf\", \"chubb.pdf\", \"pgr.pdf\", \"trv.pdf\"]\n",
    "\n",
    "documents = []  # Each document is a dict with id, source, text, and company.\n",
    "\n",
    "def extract_company_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract company code from filename.\n",
    "    Example: 'all.pdf' -> 'ALL'\n",
    "            'chubb.pdf' -> 'CHUBB'\n",
    "            'pgr.pdf' -> 'PGR'\n",
    "            'trv.pdf' -> 'TRV'\n",
    "    \"\"\"\n",
    "    # Remove the .pdf extension and convert to uppercase\n",
    "    company = filename.replace('.pdf', '').upper()\n",
    "    \n",
    "    # Map company codes (if you need specific mappings)\n",
    "    company_mapping = {\n",
    "        'ALL': 'ALL', \n",
    "        'CHUBB': 'CHUBB',\n",
    "        'PGR': 'PGR',\n",
    "        'TRV': 'TRV'\n",
    "    }\n",
    "    \n",
    "    return company_mapping.get(company, company)\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    if os.path.exists(pdf_file):\n",
    "        company = extract_company_from_filename(pdf_file)  # e.g., \"ALL\", \"CHUBB\", \"PGR\", \"TRV\"\n",
    "        text = extract_text_from_pdf(pdf_file)\n",
    "        chunks = chunk_text(text)\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            documents.append({\n",
    "                \"id\": f\"{company}_{idx}\",\n",
    "                \"source\": pdf_file,\n",
    "                \"text\": chunk,\n",
    "                \"company\": company,\n",
    "            })\n",
    "\n",
    "print(f\"Total number of chunks from all PDFs: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb0314-9c3c-4ad2-a115-d526b06921c4",
   "metadata": {},
   "source": [
    "Next, we generate embeddings for each chunk and add them to our document records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "375ff9cd-5c03-4c66-b8bc-6552476f486c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1 of 2\n",
      "Processed batch 2 of 2\n",
      "Embeddings generated for all chunks.\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(texts):\n",
    "    \"\"\"\n",
    "    Get embeddings for text using OpenAI's API.\n",
    "    Uses text-embedding-ada-002 model which returns 1536-dimensional embeddings.\n",
    "    \"\"\"\n",
    "    # If input is a single string, convert to list\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "        single_input = True\n",
    "    else:\n",
    "        single_input = False\n",
    "    \n",
    "    # Process in batches of 100\n",
    "    batch_size = 100\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        response = client.embeddings.create(\n",
    "            input=batch,\n",
    "            model=\"text-embedding-ada-002\"\n",
    "        )\n",
    "        batch_embeddings = [data.embedding for data in response.data]\n",
    "        embeddings.extend(batch_embeddings)\n",
    "        print(f\"Processed batch {i//batch_size + 1} of {math.ceil(len(texts)/batch_size)}\")\n",
    "    \n",
    "    # If input was single string, return single embedding\n",
    "    if single_input:\n",
    "        return embeddings[0]\n",
    "    return embeddings\n",
    "\n",
    "# Then modify the embedding generation loop:\n",
    "texts = [doc[\"text\"] for doc in documents]\n",
    "embeddings = get_embedding(texts)\n",
    "\n",
    "for doc, embedding in zip(documents, embeddings):\n",
    "    doc[\"vector\"] = embedding\n",
    "\n",
    "print(\"Embeddings generated for all chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628980ba-a399-4649-846c-b9f535c9ba0e",
   "metadata": {},
   "source": [
    "This code sets up a LanceDB database with a specific schema for storing document data, including text and vector embeddings, by first removing any existing table and then creating a new one populated with the previously processed PDF document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9fb6b05e-974d-48df-9d96-d0f7cbf6afe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table 'documents' with 161 rows.\n"
     ]
    }
   ],
   "source": [
    "# Connect to your LanceDB database.\n",
    "db = lancedb.connect(\"lancedb_insurance_competition\")\n",
    "\n",
    "# Define the PyArrow schema for our documents table.\n",
    "schema = pa.schema([\n",
    "    (\"id\", pa.string()),\n",
    "    (\"source\", pa.string()),\n",
    "    (\"text\", pa.string()),\n",
    "    (\"vector\", pa.list_(pa.float32(), 1536)),  # fixed size dimension 1536 for the embedding\n",
    "    (\"company\", pa.string()),\n",
    "])\n",
    "\n",
    "table_name = \"documents\"\n",
    "\n",
    "# Overwrite any existing table by dropping it.\n",
    "try:\n",
    "    db.drop_table(table_name)\n",
    "except Exception:\n",
    "    # If the table doesn't exist, ignore the error.\n",
    "    pass\n",
    "\n",
    "if documents:\n",
    "    table = db.create_table(table_name, data=documents, schema=schema)\n",
    "    print(f\"Created table '{table_name}' with {len(documents)} rows.\")\n",
    "else:\n",
    "    print(\"No documents available to create the table.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334e1b0b-486b-4493-b69c-cca0f42da831",
   "metadata": {},
   "source": [
    "extract_companies_from_query(): Identifies which insurance companies are mentioned in the user's query by looking for specific name variations (like finding both \"TRAVELERS\" and \"TRV\" to refer to the same company), ensuring accurate document retrieval for exactly the companies the user is asking about.\n",
    "\n",
    "search_lancedb(): Performs semantic search in the database by company name, with special handling for multi-company queries, returning the most relevant document chunks based on vector similarity.\n",
    "\n",
    "generate_answer(): Creates responses using retrieved document chunks and OpenAI's API, specifically formatting the prompt to address each mentioned company separately in the answer.\n",
    "\n",
    "answer_query(): The main orchestration function that ties everything together - it processes the query, finds relevant documents, and generates the final answer using the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "353e4393-1d14-40b3-97d3-a470978018fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_companies_from_query(query):\n",
    "    \"\"\"\n",
    "    Extract a list of canonical company identifiers from the query using simple substring matching.\n",
    "    \n",
    "    The mapping is as follows:\n",
    "      - \"ALL\": [\"ALL\", \"ALLSTATE\"]\n",
    "      - \"CHUBB\": [\"CHUBB\"]\n",
    "      - \"PGR\": [\"PGR\", \"PROGRESSIVE\"]\n",
    "      - \"TRV\": [\"TRV\", \"TVR\", \"TRAVELERS\", \"TRAVELER\"]\n",
    "    \"\"\"\n",
    "    query_upper = query.upper()\n",
    "    company_variants = {\n",
    "        \"ALL\": [\"ALL\", \"ALLSTATE\"],\n",
    "        \"CHUBB\": [\"CHUBB\"],\n",
    "        \"PGR\": [\"PGR\", \"PROGRESSIVE\"],\n",
    "        \"TRV\": [\"TRV\", \"TVR\", \"TRAVELERS\", \"TRAVELER\"],\n",
    "    }\n",
    "    found = set()\n",
    "    for canonical, variants in company_variants.items():\n",
    "        for variant in variants:\n",
    "            if variant in query_upper:\n",
    "                found.add(canonical)\n",
    "                break\n",
    "    return list(found)\n",
    "\n",
    "def search_lancedb(query_embedding, query, k=10):\n",
    "    # Check for group retrieval indicators.\n",
    "    all_indicators = [\n",
    "        \"all companies\",\n",
    "        \"all carriers\",\n",
    "        \"all insurers\",\n",
    "        \"all firms\",\n",
    "        \"all organizations\",\n",
    "        \"all providers\"\n",
    "    ]\n",
    "    \n",
    "    query_lower = query.lower()\n",
    "    if any(indicator in query_lower for indicator in all_indicators):\n",
    "        companies = [\"ALL\", \"CHUBB\", \"PGR\", \"TRV\"]\n",
    "        print(\"Group retrieval detected. Using companies:\", companies)\n",
    "    else:\n",
    "        companies = extract_companies_from_query(query)\n",
    "        print(\"Extracted companies:\", companies)\n",
    "    \n",
    "    if companies and len(companies) > 1:\n",
    "        results_list = []\n",
    "        for comp in companies:\n",
    "            print(f\"Searching for {comp}...\")\n",
    "            res = table.search(query_embedding)\\\n",
    "                       .where(f\"company = '{comp}'\", prefilter=True)\\\n",
    "                       .limit(k)\\\n",
    "                       .to_pandas()\n",
    "            print(f\"Found {len(res)} chunks for {comp}\")\n",
    "            results_list.append(res)\n",
    "        results = pd.concat(results_list, ignore_index=True) if results_list else pd.DataFrame()\n",
    "    elif companies:\n",
    "        print(f\"Searching for {companies[0]}...\")\n",
    "        results = table.search(query_embedding)\\\n",
    "                       .where(f\"company = '{companies[0]}'\", prefilter=True)\\\n",
    "                       .limit(k)\\\n",
    "                       .to_pandas()\n",
    "        print(f\"Found {len(results)} chunks for {companies[0]}\")\n",
    "    else:\n",
    "        print(\"No company filter applied.\")\n",
    "        results = table.search(query_embedding).limit(k).to_pandas()\n",
    "    return results\n",
    "\n",
    "def generate_answer(query, retrieved_chunks, model=\"o3-mini\"):\n",
    "    \"\"\"\n",
    "    Generate an answer using the OpenAI chat completions API,\n",
    "    providing the retrieved context as part of the prompt.\n",
    "    \n",
    "    The prompt instructs the assistant to address each company mentioned in the question separately.\n",
    "    \"\"\"\n",
    "    context_text = \"\\n\\n\".join(retrieved_chunks)\n",
    "    prompt = f\"\"\"You are an insurance competitive intelligence assistant.\n",
    "\n",
    "Below is context extracted from financial statements of multiple companies.\n",
    "Please answer the following question by addressing each company mentioned in the question separately.\n",
    "If context for a company is missing, state that the relevant information is not available.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "    )\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    return answer\n",
    "\n",
    "def answer_query(query, top_k=10):\n",
    "    \"\"\"\n",
    "    Process the query by computing its embedding, searching for relevant document chunks\n",
    "    (with filtering based on detected companies), and generating an answer using the retrieved chunks.\n",
    "    \n",
    "    Returns a tuple (answer, retrieved_chunks).\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    results_df = search_lancedb(query_embedding, query, k=top_k)\n",
    "    if results_df.empty:\n",
    "        return \"No relevant context found for your query.\", []\n",
    "    retrieved_chunks = results_df[\"text\"].tolist() if \"text\" in results_df.columns else []\n",
    "    answer = generate_answer(query, retrieved_chunks)\n",
    "    return answer, retrieved_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7b375e-c46e-40ff-9e02-457c8b5297a7",
   "metadata": {},
   "source": [
    "this cell creates a basic UI where you can try it on your local jupyter notebook instance to verify that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "942c85eb-d4db-47d7-bfab-a257c42d3a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ca7580b8b340658d5c51df36c932e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='Query:', layout=Layout(width='80%'), placeholder='Enter your query …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter your query here, e.g., \"What is PGR revenue?\"',\n",
    "    description='Query:',\n",
    "    layout={'width': '80%'}\n",
    ")\n",
    "submit_button = widgets.Button(description=\"Submit Query\")\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_submit(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output()  # Clear previous output.\n",
    "        query = query_input.value\n",
    "        print(\"Your query:\", query)\n",
    "        try:\n",
    "            answer, context = answer_query(query)\n",
    "            print(\"Response:\", answer)\n",
    "            if context:\n",
    "                print(\"\\nRetrieved Context Chunks:\")\n",
    "                for idx, chunk in enumerate(context, 1):\n",
    "                    print(f\"Chunk {idx}:\", chunk)\n",
    "        except Exception as e:\n",
    "            print(\"Error processing query:\", e)\n",
    "\n",
    "submit_button.on_click(on_submit)\n",
    "display(widgets.VBox([query_input, submit_button, output_area]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db7f024-7264-4080-b1da-e75f6187a2d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
