{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d804ed0e-c6b0-41ae-a9a3-7040fcc6dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "!pip install PyPDF2 openai lancedb ipywidgets\n",
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import lancedb\n",
    "from ipywidgets import Text, Button, VBox, Output\n",
    "from openai import OpenAI\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = \"ENTER_YOUR_API_KEY\"\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a5f605-8497-48a9-ac50-25ae5e4f49ae",
   "metadata": {},
   "source": [
    "Extracts text from a PDF file by reading each page sequentially and combines it into a single string, then splits this text into overlapping chunks of specified size for easier processing or analysis.\n",
    "\n",
    "Insert your own OpenAI key above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b31e09b-3ac2-4a85-b3d7-856cfc3516a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "def extract_text_from_pdf(file_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = min(start + chunk_size, len(words))\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap  # move start with overlap\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eadebe7-55ba-4efd-853e-f52a13623717",
   "metadata": {},
   "source": [
    "This code processes multiple PDF files by extracting text from each, chunking the content, and organizing it into a structured list of dictionaries where each dictionary contains a unique identifier, source file, text chunk, and company code derived from the filename.\n",
    "\n",
    "If you don't have the documents, you can go to sec.gov > view as html > print > save as pdf (https://www.sec.gov/ix?doc=/Archives/edgar/data/0000080661/000008066124000007/pgr-20231231.htm). Important to keep names as described in the files list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3fedb07-a06d-4f8e-834c-b803dee822e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks from all PDFs: 861\n"
     ]
    }
   ],
   "source": [
    "# List of expected PDF filenames and their corresponding company codes will be derived from the filename.\n",
    "pdf_files = [\"all.pdf\", \"chubb.pdf\", \"pgr.pdf\", \"trv.pdf\"]\n",
    "\n",
    "documents = []  # Each document is a dict with id, source, text, and company.\n",
    "\n",
    "def extract_company_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract company code from filename.\n",
    "    Example: 'all.pdf' -> 'ALL'\n",
    "            'chubb.pdf' -> 'CHUBB'\n",
    "            'pgr.pdf' -> 'PGR'\n",
    "            'trv.pdf' -> 'TRV'\n",
    "    \"\"\"\n",
    "    # Remove the .pdf extension and convert to uppercase\n",
    "    company = filename.replace('.pdf', '').upper()\n",
    "    \n",
    "    # Map company codes (if you need specific mappings)\n",
    "    company_mapping = {\n",
    "        'ALL': 'ALL',\n",
    "        'CHUBB': 'CHUBB',\n",
    "        'PGR': 'PGR',\n",
    "        'TRV': 'TRV'\n",
    "    }\n",
    "    \n",
    "    return company_mapping.get(company, company)\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    if os.path.exists(pdf_file):\n",
    "        company = extract_company_from_filename(pdf_file)  # e.g., \"ALL\", \"CHUBB\", \"PGR\", \"TRV\"\n",
    "        text = extract_text_from_pdf(pdf_file)\n",
    "        chunks = chunk_text(text)\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            documents.append({\n",
    "                \"id\": f\"{company}_{idx}\",\n",
    "                \"source\": pdf_file,\n",
    "                \"text\": chunk,\n",
    "                \"company\": company,\n",
    "            })\n",
    "\n",
    "print(f\"Total number of chunks from all PDFs: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb0314-9c3c-4ad2-a115-d526b06921c4",
   "metadata": {},
   "source": [
    "Next, we generate embeddings for each chunk and add them to our document records.\n",
    "\n",
    "Warning: compute-intensive code, takes ~5 minutes to generate embeddings for 35MB of pdfs on M1 Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "375ff9cd-5c03-4c66-b8bc-6552476f486c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated for all chunks.\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "# Generate embeddings for all chunks using the new client API\n",
    "def get_embedding(text):\n",
    "    \"\"\"\n",
    "    Get embeddings for text using OpenAI's API.\n",
    "    Uses text-embedding-ada-002 model which returns 1536-dimensional embeddings.\n",
    "    \"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    # Extract the embedding from the response\n",
    "    return response.data[0].embedding\n",
    "    \n",
    "for doc in documents:\n",
    "    # Note: We now store the vector under the key \"vector\"\n",
    "    doc[\"vector\"] = get_embedding(doc[\"text\"])\n",
    "\n",
    "print(\"Embeddings generated for all chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628980ba-a399-4649-846c-b9f535c9ba0e",
   "metadata": {},
   "source": [
    "This code sets up a LanceDB database with a specific schema for storing document data, including text and vector embeddings, by first removing any existing table and then creating a new one populated with the previously processed PDF document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fb6b05e-974d-48df-9d96-d0f7cbf6afe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table 'documents' with 861 rows.\n"
     ]
    }
   ],
   "source": [
    "# Connect to your LanceDB database.\n",
    "db = lancedb.connect(\"lancedb_insurance_competition\")\n",
    "\n",
    "# Define the PyArrow schema for our documents table.\n",
    "schema = pa.schema([\n",
    "    (\"id\", pa.string()),\n",
    "    (\"source\", pa.string()),\n",
    "    (\"text\", pa.string()),\n",
    "    (\"vector\", pa.list_(pa.float32(), 1536)),  # fixed size dimension 1536 for the embedding\n",
    "    (\"company\", pa.string()),\n",
    "])\n",
    "\n",
    "table_name = \"documents\"\n",
    "\n",
    "# Overwrite any existing table by dropping it.\n",
    "try:\n",
    "    db.drop_table(table_name)\n",
    "except Exception:\n",
    "    # If the table doesn't exist, ignore the error.\n",
    "    pass\n",
    "\n",
    "if documents:\n",
    "    table = db.create_table(table_name, data=documents, schema=schema)\n",
    "    print(f\"Created table '{table_name}' with {len(documents)} rows.\")\n",
    "else:\n",
    "    print(\"No documents available to create the table.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334e1b0b-486b-4493-b69c-cca0f42da831",
   "metadata": {},
   "source": [
    "extract_companies_from_query(): Identifies which insurance companies are mentioned in the user's query by looking for specific name variations (like finding both \"TRAVELERS\" and \"TRV\" to refer to the same company), ensuring accurate document retrieval for exactly the companies the user is asking about.\n",
    "\n",
    "search_lancedb(): Performs semantic search in the database by company name, with special handling for multi-company queries, returning the most relevant document chunks based on vector similarity.\n",
    "\n",
    "generate_answer(): Creates responses using retrieved document chunks and OpenAI's API, specifically formatting the prompt to address each mentioned company separately in the answer.\n",
    "\n",
    "answer_query(): The main orchestration function that ties everything together - it processes the query, finds relevant documents, and generates the final answer using the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "353e4393-1d14-40b3-97d3-a470978018fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_companies_from_query(query):\n",
    "    \"\"\"\n",
    "    Extract a list of canonical company identifiers from the query using simple substring matching.\n",
    "    \n",
    "    The mapping is as follows:\n",
    "      - \"ALL\": [\"ALLSTATE\"]    # Instead of [\"ALL\", \"ALLSTATE\"]\n",
    "      - \"CHUBB\": [\"CHUBB\"]\n",
    "      - \"PGR\": [\"PGR\", \"PROGRESSIVE\"]\n",
    "      - \"TRV\": [\"TRV\", \"TVR\", \"TRAVELERS\", \"TRAVELER\"]\n",
    "    \"\"\"\n",
    "    query_upper = query.upper()\n",
    "    company_variants = {\n",
    "        \"ALL\": [\"ALLSTATE\"],    # Require the full word \"ALLSTATE\"\n",
    "        \"CHUBB\": [\"CHUBB\"],\n",
    "        \"PGR\": [\"PGR\", \"PROGRESSIVE\"],\n",
    "        \"TRV\": [\"TRV\", \"TVR\", \"TRAVELERS\", \"TRAVELER\"],\n",
    "    }\n",
    "    found = set()\n",
    "    for canonical, variants in company_variants.items():\n",
    "        for variant in variants:\n",
    "            if variant in query_upper:\n",
    "                found.add(canonical)\n",
    "                break\n",
    "    return list(found)\n",
    "\n",
    "def search_lancedb(query_embedding, query, k=10):\n",
    "    \"\"\"\n",
    "    Search the LanceDB table for the top k documents similar to the query embedding.\n",
    "    \n",
    "    If the query mentions multiple companies, perform a separate search for each and combine the results.\n",
    "    \"\"\"\n",
    "    companies = extract_companies_from_query(query)\n",
    "    print(\"Extracted companies from query:\", companies)\n",
    "    \n",
    "    if companies and len(companies) > 1:\n",
    "        results_list = []\n",
    "        for comp in companies:\n",
    "            print(f\"Searching for {comp}...\")\n",
    "            res = table.search(query_embedding)\\\n",
    "                       .where(f\"company = '{comp}'\", prefilter=True)\\\n",
    "                       .limit(k)\\\n",
    "                       .to_pandas()\n",
    "            print(f\"Found {len(res)} chunks for {comp}\")\n",
    "            results_list.append(res)\n",
    "        if results_list:\n",
    "            results = pd.concat(results_list, ignore_index=True)\n",
    "        else:\n",
    "            results = pd.DataFrame()\n",
    "    elif companies:\n",
    "        print(f\"Searching for {companies[0]}...\")\n",
    "        results = table.search(query_embedding)\\\n",
    "                       .where(f\"company = '{companies[0]}'\", prefilter=True)\\\n",
    "                       .limit(k)\\\n",
    "                       .to_pandas()\n",
    "        print(f\"Found {len(results)} chunks for {companies[0]}\")\n",
    "    else:\n",
    "        print(\"No company filter applied.\")\n",
    "        results = table.search(query_embedding).limit(k).to_pandas()\n",
    "    return results\n",
    "\n",
    "def generate_answer(query, retrieved_chunks, model=\"o3-mini\"):\n",
    "    \"\"\"\n",
    "    Generate an answer using the OpenAI chat completions API,\n",
    "    providing the retrieved context as part of the prompt.\n",
    "    \n",
    "    The prompt instructs the assistant to address each company mentioned in the question separately.\n",
    "    \"\"\"\n",
    "    context_text = \"\\n\\n\".join(retrieved_chunks)\n",
    "    prompt = f\"\"\"You are an insurance competitive intelligence assistant.\n",
    "\n",
    "Below is context extracted from financial statements of multiple companies.\n",
    "Please answer the following question by addressing each company mentioned in the question separately.\n",
    "If context for a company is missing, state that the relevant information is not available.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "    )\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    return answer\n",
    "\n",
    "def answer_query(query, top_k=10):\n",
    "    \"\"\"\n",
    "    Process the query by computing its embedding, searching for relevant document chunks\n",
    "    (with filtering based on detected companies), and generating an answer using the retrieved chunks.\n",
    "    \n",
    "    Returns a tuple (answer, retrieved_chunks).\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    results_df = search_lancedb(query_embedding, query, k=top_k)\n",
    "    if results_df.empty:\n",
    "        return \"No relevant context found for your query.\", []\n",
    "    retrieved_chunks = results_df[\"text\"].tolist() if \"text\" in results_df.columns else []\n",
    "    answer = generate_answer(query, retrieved_chunks)\n",
    "    return answer, retrieved_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7b375e-c46e-40ff-9e02-457c8b5297a7",
   "metadata": {},
   "source": [
    "this cell creates a basic UI where you can try it on your local jupyter notebook instance to verify that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "942c85eb-d4db-47d7-bfab-a257c42d3a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547281afcb844bedbfd50d73801c0418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='Query:', layout=Layout(width='80%'), placeholder='Enter your query …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter your query here, e.g., \"What is PGR revenue?\"',\n",
    "    description='Query:',\n",
    "    layout={'width': '80%'}\n",
    ")\n",
    "submit_button = widgets.Button(description=\"Submit Query\")\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_submit(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output()  # Clear previous output.\n",
    "        query = query_input.value\n",
    "        print(\"Your query:\", query)\n",
    "        try:\n",
    "            answer, context = answer_query(query)\n",
    "            print(\"Response:\", answer)\n",
    "            if context:\n",
    "                print(\"\\nRetrieved Context Chunks:\")\n",
    "                for idx, chunk in enumerate(context, 1):\n",
    "                    print(f\"Chunk {idx}:\", chunk)\n",
    "        except Exception as e:\n",
    "            print(\"Error processing query:\", e)\n",
    "\n",
    "submit_button.on_click(on_submit)\n",
    "display(widgets.VBox([query_input, submit_button, output_area]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5685806-7773-45ad-bf5a-e7e59ed821d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
