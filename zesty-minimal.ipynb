{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d8f5b38-1b41-4c53-8443-64fcbeb6b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "%pip install PyPDF2 openai lancedb \n",
    "\n",
    "import os\n",
    "import math\n",
    "import PyPDF2\n",
    "import lancedb\n",
    "import pyarrow as pa\n",
    "from openai import OpenAI\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = \"ENTER_YOUR_API_KEY\"\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f46724a-ad74-4b1c-9684-9ec7e036c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    print(f\"Processing {file_path}...\")\n",
    "    text = \"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        print(f\"- Found {len(reader.pages)} pages\")\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "            if (page_num + 1) % 20 == 0:  # Progress every 20 pages\n",
    "                print(f\"- Processed {page_num + 1} pages\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b782ab31-0e86-4aa6-b30a-685ce14c87e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=4000, overlap=256):\n",
    "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "    text_splitter = TokenTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        encoding_name=\"cl100k_base\"\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37b2e53d-566e-459a-beef-fa72b23c86e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_company_from_filename(filename):\n",
    "    \"\"\"Extract company code from filename.\"\"\"\n",
    "    company = filename.replace('.pdf', '').upper()\n",
    "    company_mapping = {\n",
    "        'ALL': 'ALL', \n",
    "        'CHUBB': 'CHUBB',\n",
    "        'PGR': 'PGR',\n",
    "        'TRV': 'TRV'\n",
    "    }\n",
    "    return company_mapping.get(company, company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78ce1f77-034f-4ab5-b8ec-7a4947e3f713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(texts):\n",
    "    \"\"\"Get embeddings for text using OpenAI's API.\"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "        single_input = True\n",
    "    else:\n",
    "        single_input = False\n",
    "    \n",
    "    batch_size = 100\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        batch_end = min(i + batch_size, len(texts))\n",
    "        print(f\"Processing embeddings for batch {i//batch_size + 1}, documents {i} to {batch_end-1}\")\n",
    "        \n",
    "        response = client.embeddings.create(\n",
    "            input=batch,\n",
    "            model=\"text-embedding-ada-002\"\n",
    "        )\n",
    "        batch_embeddings = [data.embedding for data in response.data]\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    return embeddings[0] if single_input else embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79fab0ee-9043-4eeb-8950-7885eb5f11d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing all.pdf...\n",
      "- Found 251 pages\n",
      "- Processed 20 pages\n",
      "- Processed 40 pages\n",
      "- Processed 60 pages\n",
      "- Processed 80 pages\n",
      "- Processed 100 pages\n",
      "- Processed 120 pages\n",
      "- Processed 140 pages\n",
      "- Processed 160 pages\n",
      "- Processed 180 pages\n",
      "- Processed 200 pages\n",
      "- Processed 220 pages\n",
      "- Processed 240 pages\n",
      "Created 50 chunks\n",
      "Processing all.pdf: 50 chunks\n",
      "Processing chubb.pdf...\n",
      "- Found 356 pages\n",
      "- Processed 20 pages\n",
      "- Processed 40 pages\n",
      "- Processed 60 pages\n",
      "- Processed 80 pages\n",
      "- Processed 100 pages\n",
      "- Processed 120 pages\n",
      "- Processed 140 pages\n",
      "- Processed 160 pages\n",
      "- Processed 180 pages\n",
      "- Processed 200 pages\n",
      "- Processed 220 pages\n",
      "- Processed 240 pages\n",
      "- Processed 260 pages\n",
      "- Processed 280 pages\n",
      "- Processed 300 pages\n",
      "- Processed 320 pages\n",
      "- Processed 340 pages\n",
      "Created 53 chunks\n",
      "Processing chubb.pdf: 53 chunks\n",
      "Processing pgr.pdf...\n",
      "- Found 79 pages\n",
      "- Processed 20 pages\n",
      "- Processed 40 pages\n",
      "- Processed 60 pages\n",
      "Created 12 chunks\n",
      "Processing pgr.pdf: 12 chunks\n",
      "Processing trv.pdf...\n",
      "- Found 244 pages\n",
      "- Processed 20 pages\n",
      "- Processed 40 pages\n",
      "- Processed 60 pages\n",
      "- Processed 80 pages\n",
      "- Processed 100 pages\n",
      "- Processed 120 pages\n",
      "- Processed 140 pages\n",
      "- Processed 160 pages\n",
      "- Processed 180 pages\n",
      "- Processed 200 pages\n",
      "- Processed 220 pages\n",
      "- Processed 240 pages\n",
      "Created 46 chunks\n",
      "Processing trv.pdf: 46 chunks\n",
      "Total chunks: 161\n",
      "Processing embeddings for batch 1, documents 0 to 99\n",
      "Processing embeddings for batch 2, documents 100 to 160\n",
      "Created database with 161 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-07T05:23:17Z WARN  lance::dataset::write::insert] No existing dataset at /Users/andrey/Dev/zesty/lancedb_insurance_competition/documents.lance, it will be created\n"
     ]
    }
   ],
   "source": [
    "def prepare_database():\n",
    "    # Process all PDFs first\n",
    "    pdf_files = [\"all.pdf\", \"chubb.pdf\", \"pgr.pdf\", \"trv.pdf\"]\n",
    "    documents = []\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        if not os.path.exists(pdf_file):\n",
    "            print(f\"File not found: {pdf_file}\")\n",
    "            continue\n",
    "            \n",
    "        company = pdf_file.replace('.pdf', '').upper()\n",
    "        text = extract_text_from_pdf(pdf_file)\n",
    "        chunks = chunk_text(text)\n",
    "        print(f\"Processing {pdf_file}: {len(chunks)} chunks\")\n",
    "        \n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            documents.append({\n",
    "                \"id\": f\"{company}_{idx}\",\n",
    "                \"source\": pdf_file,\n",
    "                \"text\": chunk,\n",
    "                \"company\": company,\n",
    "            })\n",
    "\n",
    "    print(f\"Total chunks: {len(documents)}\")\n",
    "    \n",
    "    # Get embeddings for all texts at once\n",
    "    texts = [doc[\"text\"] for doc in documents]\n",
    "    embeddings = get_embedding(texts)\n",
    "    \n",
    "    # Add embeddings to documents\n",
    "    for doc, embedding in zip(documents, embeddings):\n",
    "        doc[\"vector\"] = embedding\n",
    "\n",
    "    # Create table with explicit schema definition for fixed-size list\n",
    "    db = lancedb.connect(\"lancedb_insurance_competition\")\n",
    "    table_name = \"documents\"\n",
    "    \n",
    "    try:\n",
    "        db.drop_table(table_name)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Define schema with explicit list_size for vector\n",
    "    schema = pa.schema([\n",
    "        (\"id\", pa.string()),\n",
    "        (\"source\", pa.string()),\n",
    "        (\"text\", pa.string()),\n",
    "        (\"vector\", pa.list_(pa.float32(), 1536)),  # Fixed size list with 1536 dimensions\n",
    "        (\"company\", pa.string()),\n",
    "    ])\n",
    "\n",
    "    # Create Arrow Table with correct schema\n",
    "    arrays = [\n",
    "        pa.array([doc[\"id\"] for doc in documents]),\n",
    "        pa.array([doc[\"source\"] for doc in documents]),\n",
    "        pa.array([doc[\"text\"] for doc in documents]),\n",
    "        pa.FixedSizeListArray.from_arrays(\n",
    "            pa.array([v for doc in documents for v in doc[\"vector\"]], type=pa.float32()),\n",
    "            1536\n",
    "        ),\n",
    "        pa.array([doc[\"company\"] for doc in documents])\n",
    "    ]\n",
    "    arrow_table = pa.Table.from_arrays(arrays, schema=schema)\n",
    "\n",
    "    # Create LanceDB table from Arrow Table\n",
    "    table = db.create_table(table_name, data=arrow_table, mode=\"overwrite\")\n",
    "    print(f\"Created database with {len(documents)} documents\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prepare_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "948b1afa-0c40-410d-b8a8-5eb0d5482d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Schema:\n",
      "id: string\n",
      "source: string\n",
      "text: string\n",
      "vector: fixed_size_list<item: float>[1536]\n",
      "  child 0, item: float\n",
      "company: string\n",
      "\n",
      "Total row count reported by DB: 161\n",
      "\n",
      "Retrieved row count: 161\n",
      "\n",
      "Success: All data is present in the table.\n",
      "\n",
      "Documents per company:\n",
      "company\n",
      "CHUBB    53\n",
      "ALL      50\n",
      "TRV      46\n",
      "PGR      12\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import lancedb\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Connect to the database ---\n",
    "db = lancedb.connect(\"lancedb_insurance_competition\")\n",
    "\n",
    "# --- 2. Open the table ---\n",
    "table = db.open_table(\"documents\")\n",
    "\n",
    "# --- 3. Print the table schema (structure) ---\n",
    "print(\"Table Schema:\")\n",
    "print(table.schema)\n",
    "\n",
    "# --- 4. Get the total row count reported by the table ---\n",
    "reported_count = table.count_rows()\n",
    "print(\"\\nTotal row count reported by DB:\", reported_count)\n",
    "\n",
    "# --- 5. Retrieve all rows using the search interface with a high limit ---\n",
    "# Using search(None) tells LanceDB you want all rows.\n",
    "full_data = table.search(None).limit(1000).to_pandas()\n",
    "retrieved_count = len(full_data)\n",
    "print(\"\\nRetrieved row count:\", retrieved_count)\n",
    "\n",
    "if retrieved_count == reported_count:\n",
    "    print(\"\\nSuccess: All data is present in the table.\")\n",
    "else:\n",
    "    print(f\"\\nWarning: Expected {reported_count} rows but retrieved {retrieved_count} rows.\")\n",
    "\n",
    "# --- 6. Show how many documents exist for each company ---\n",
    "print(\"\\nDocuments per company:\")\n",
    "company_counts = full_data['company'].value_counts()\n",
    "print(company_counts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
